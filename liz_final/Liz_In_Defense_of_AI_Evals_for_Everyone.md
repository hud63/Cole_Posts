The "we don't need AI evals" crowd is lying to themselves.

Every successful AI product does evaluations. They just don't call it that.

ðŸ‘‰ Looking at outputs and thinking "this feels off"? That's evaluation.

ðŸ‘‰ Dogfooding your product religiously? That's evaluation.

ðŸ‘‰ Making changes based on what you observe? That's systematic measurement.

The anti-eval sentiment is particularly damaging for newcomers to AI who don't have years of domain expertise to "steer by feel."

**Here's the truth nobody wants to admit:**

Foundation model companies spend BILLIONS on rigorous evaluation. Scale, Snorkel, Mercor - all valued in the billions specifically for helping with evals.

Meanwhile, indie builders reject the same systematic approaches that made these models work in the first place.

**The real problem isn't evals being too rigid.**

The real problem is founders thinking they can skip the fundamentals that separate products that endure from those that disappear.

ðŸ‘‰ You can't optimize what you don't measure

ðŸ‘‰ You can't improve what you don't understand

ðŸ‘‰ You can't scale what you haven't systematized

Stop pretending you don't do evals.

Start admitting you do them poorly.